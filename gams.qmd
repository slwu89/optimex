---
title: "IJKLM model"
author: "Sean L. Wu"
date: "11/13/2023"
format: gfm
jupyter: julia-1.9
---

## The IJKLM model

An (in)famous [Julia Discourse thread](https://discourse.julialang.org/t/performance-julia-jump-vs-python-pyomo/92044) once asked how to make model building in JuMP faster compared to other platforms (Pyomo, and I guess GAMS). This turned into a [blog post by GAMS](https://www.gams.com/blog/2023/07/performance-in-optimization-models-a-comparative-analysis-of-gams-pyomo-gurobipy-and-jump/), a company which produces a modeling language that plugs into various commercial and open source solvers. The original formulation of the JuMP model showed very bad performance, but that was due to the way the author (employee of GAMS) was using Julia, rather than anything JuMP specific. The JuMP dev team responded with their own [blog post on the JuMP website](https://jump.dev/2023/07/20/gams-blog/), which was announced on a [Discourse thread](https://discourse.julialang.org/t/jump-developers-response-to-benchmarks-by-gams/101920). They used a DataFrames.jl based solution that was very fast. 

The backstory is filled with intrigue, no doubt, but I'm interested to see if acsets can provide an alterative way to generate this model. The original data is reproducible at [justine18/performance_experiment](https://github.com/justine18/performance_experiment), but I decided to just redo it in Julia as the writing/reading to/from JSON files is a pain in the butt.

By the way, the model is given as:

$$\text{min} \ z = 1$$

$$\sum_{(j,k):(i,j,k) \in \mathcal{IJK}} \ \sum_{l:(j,k,l) \in \mathcal{JKL}} \ \sum_{m:(k,l,m) \in \mathcal{KLM}} x_{i,j,k,l,m} \ge 0 \hspace{1cm} \forall \ i \in \mathcal{I}$$

$$x_{i,j,k,l,m} \ge 0 \hspace{1cm} \forall \ (i,j,k) \in \mathcal{IJK}, l:(j,k,l) \in \mathcal{JKL}, m:(k,l,m) \in \mathcal{KLM} $$

The blog post calls subsets of Cartesian products "maps", which seems to be confused as a "map" is generally understood to be a function in math. General subsets of products are known as "relations".

## Data generation

First we load some packages we'll need. `DataFrames` for data frames, `Distributions` for sampling binomial random variates, `JuMP` to set up the model, `HiGHS` for a solver. `Catlab` provides a new data structure, the C-Set which will be compared to the data frames method.

```{julia}
#| output: false
#| results: false
using DataFrames
using Distributions
using JuMP, HiGHS
using Catlab
using BenchmarkTools, MarkdownTables
```

We first generate synthetic "data". This should follow the data generation as I understand it from the original repo. The probability of all zeros with the given model sizes is incomprehensibly small but I added a check for it anyway. Who knows.

```{julia}
#| output: false
SampleBinomialVec = function(A,B,C,p=0.05)
    vec = rand(Binomial(1, p), length(A) * length(B) * length(C))
    while sum(vec) == 0
        vec = rand(Binomial(1, p), length(A) * length(B) * length(C))
    end
    return vec
end

n=100 # something large
m=20 # 20

# Sets IJKLM 
I = ["i$x" for x in 1:n]
J = ["j$x" for x in 1:m]
K = ["k$x" for x in 1:m]
L = ["l$x" for x in 1:m]
M = ["m$x" for x in 1:m]

# make IJK
IJK = DataFrame(Iterators.product(I,J,K))
rename!(IJK, [:i,:j,:k])
IJK.value = SampleBinomialVec(I,J,K)

# make JKL
JKL = DataFrame(Iterators.product(J,K,L))
rename!(JKL, [:j,:k,:l])
JKL.value = SampleBinomialVec(J,K,L)

# make KLM
KLM = DataFrame(Iterators.product(K,L,M))
rename!(KLM, [:k,:l,:m])
KLM.value = SampleBinomialVec(K,L,M)

# make the products just general sparse relations
IJK_sparse = [(x.i, x.j, x.k) for x in eachrow(IJK) if x.value == true]
JKL_sparse = [(x.j, x.k, x.l) for x in eachrow(JKL) if x.value == true]
KLM_sparse = [(x.k, x.l, x.m) for x in eachrow(KLM) if x.value == true]
```

## The "intuitive" formulation

As we know this is the slow one.

```{julia}
@benchmark let 
    x_list = [
        (i, j, k, l, m)
        for (i, j, k) in IJK_sparse
        for (jj, kk, l) in JKL_sparse if jj == j && kk == k
        for (kkk, ll, m) in KLM_sparse if kkk == k && ll == l
    ]
    model = JuMP.Model(HiGHS.Optimizer)
    set_silent(model)
    @variable(model, x[x_list] >= 0)
    @constraint(
        model,
        [i in I], 
        sum(x[k] for k in x_list if k[1] == i) >= 0
    )
    optimize!(model)
end
```

## The DataFrames version

The fast one at the JuMP blog link.

```{julia}
IJK_sparse_df = filter(x -> x.value == 1, IJK)
select!(IJK_sparse_df, Not(:value))

JKL_sparse_df = filter(x -> x.value == 1, JKL)
select!(JKL_sparse_df, Not(:value))

KLM_sparse_df = filter(x -> x.value == 1, KLM)
select!(KLM_sparse_df, Not(:value))

ijklm_df = DataFrames.innerjoin(
    DataFrames.innerjoin(IJK_sparse_df, JKL_sparse_df; on = [:j, :k]),
    KLM_sparse_df;
    on = [:k, :l],
)
```

Let's benchmark it.

```{julia}
@benchmark let
    ijklm = DataFrames.innerjoin(
        DataFrames.innerjoin(IJK_sparse_df, JKL_sparse_df; on = [:j, :k]),
        KLM_sparse_df;
        on = [:k, :l],
    )
    model = JuMP.Model(HiGHS.Optimizer)
    set_silent(model)
    ijklm[!, :x] = @variable(model, x[1:size(ijklm, 1)] >= 0)
    for df in DataFrames.groupby(ijklm, :i)
        @constraint(model, sum(df.x) >= 0)
    end
    optimize!(model)
end
```

## The acsets version

Acsets (Attributed C-Sets) are a nifty data structure coming from applied category theory, but its not too far off to think of them just as in-memory relational databases. They are provided in the [ACSets.jl](https://github.com/AlgebraicJulia/ACSets.jl) library, and imported and extended with machinery from applied category theory in [Catlab.jl](https://github.com/AlgebraicJulia/Catlab.jl). One of the advantages of using acsets and categorical machinery in general is that they have a natural graphical presentation, which in many cases is very readable and illuminating. To learn more about this data structure and many more topics, you can consult the package documentation and also you can begin with some [introductory articles on the AlgebraicJulia blog](https://blog.algebraicjulia.org/post/2020/09/cset-graphs-1/).

We use `@present` to make a schema for the acset which will store the data. Note that each "set" has turned into an object in the schema, and that the relations are also objects. There are projection maps from the relations into the sets which are involved in each relation. Each relation also has an arrow into `IntAttr` which will store the results of the binomial random draw that is used to "sparsify" the data.

```{julia}
@present IJKLMSch(FreeSchema) begin
    (I,J,K,L,M,IJK,JKL,KLM)::Ob
    IJK_I::Hom(IJK,I)
    IJK_J::Hom(IJK,J)
    IJK_K::Hom(IJK,K)
    JKL_J::Hom(JKL,J)
    JKL_K::Hom(JKL,K)
    JKL_L::Hom(JKL,L)
    KLM_K::Hom(KLM,K)
    KLM_L::Hom(KLM,L)
    KLM_M::Hom(KLM,M)
    IntAttr::AttrType
    value_ijk::Attr(IJK,IntAttr)
    value_jkl::Attr(JKL,IntAttr)
    value_klm::Attr(KLM,IntAttr)
end

Catlab.to_graphviz(IJKLMSch, graph_attrs=Dict(:dpi=>"72",:ratio=>"expand",:size=>"8"))
```

Now we programatically generate the data type (and functions to work with it) for our schema, and fill it with data. The code is verbose, but we're storing all the sets and relations in a single data structure.

```{julia}
@acset_type IJKLMData(IJKLMSch, index=[:IJK_I,:IJK_J,:IJK_K,:JKL_J,:JKL_K,:JKL_L,:KLM_K,:KLM_L,:KLM_M])

# the basic sets
I = collect(1:n)
J = collect(1:m)
K = collect(1:m)
L = collect(1:m)
M = collect(1:m)

# make the data
ijklm_dat = IJKLMData{Int}()

add_parts!(ijklm_dat, :I, length(I))
add_parts!(ijklm_dat, :J, length(J))
add_parts!(ijklm_dat, :K, length(K))
add_parts!(ijklm_dat, :L, length(L))
add_parts!(ijklm_dat, :M, length(M))

# add the relations...first as product...then sparsify

# IJK
ijk_prod = Iterators.product(I,J,K)

add_parts!(
    ijklm_dat, 
    :IJK,
    length(ijk_prod),
    IJK_I=vec([e[1] for e in ijk_prod]),
    IJK_J=vec([e[2] for e in ijk_prod]),
    IJK_K=vec([e[3] for e in ijk_prod]),
    value_ijk=IJK.value
)

rem_parts!(
    ijklm_dat, 
    :IJK, 
    findall(ijklm_dat[:,:value_ijk] .== 0)
)

# JKL
jkl_prod = Iterators.product(J,K,L)

add_parts!(
    ijklm_dat, 
    :JKL,
    length(jkl_prod),
    JKL_J=vec([e[1] for e in jkl_prod]),
    JKL_K=vec([e[2] for e in jkl_prod]),
    JKL_L=vec([e[3] for e in jkl_prod]),
    value_jkl=JKL.value
)

rem_parts!(
    ijklm_dat, 
    :JKL, 
    findall(ijklm_dat[:,:value_jkl] .== 0)
)

# KLM
klm_prod = Iterators.product(K,L,M)

add_parts!(
    ijklm_dat, 
    :KLM,
    length(klm_prod),
    KLM_K=vec([e[1] for e in klm_prod]),
    KLM_L=vec([e[2] for e in klm_prod]),
    KLM_M=vec([e[3] for e in klm_prod]),
    value_klm=KLM.value
)

rem_parts!(
    ijklm_dat, 
    :KLM, 
    findall(ijklm_dat[:,:value_klm] .== 0)
)
```

Now, the critical thing that the JuMP devs did to speed thing up was to replace the for loops with 2 inner joins, to get the "paths" through the relations. How to do this with acsets? Well we can execute a conjunctive query on the acset to get the same thing. This is described in a [post at the AlgebraicJulia blog](https://blog.algebraicjulia.org/post/2020/12/cset-conjunctive-queries/).

```{julia}
connected_paths_query = @relation (i=i,j=j,k=k,l=l,m=m) begin
    IJK(IJK_I=i, IJK_J=j, IJK_K=k)
    JKL(JKL_J=j, JKL_K=k, JKL_L=l)
    KLM(KLM_K=k, KLM_L=l, KLM_M=m)
end

Catlab.to_graphviz(connected_paths_query, box_labels=:name, junction_labels=:variable, graph_attrs=Dict(:dpi=>"72",:size=>"3.5",:ratio=>"expand"))
```

While the blog post should be consulted for a complete explanation, the conjunctive query is expressed using an undirected wiring diagram (UWD) which is visualized using Catlab. Nodes (labeled ovals) in the UWD correspond to tables (primary keys) in the acset. Junctions (labeled dots) correspond to variables. Ports, which are unlabed in this graphical depiction, are where wires connect junctions to nodes. These correspond to columns of the table they are connected to. Outer ports, which are wires that run "off the page", are the columns of the table that will be returned as the result of the query. The rows that are returned from the query come from filtering the Cartesian product of the tables (nodes) such that variables in columns match according to ports that share a junction. In this case, it is equivalent to the two inner joins done above using DataFrames.

The JuMP blog post notes that while the data frames version doesn't resemble the nested summation it is arguably just as readable, especially if the columns were related to the process that was being modeled. I suggest that the acsets version is also just as readable, if not more, as the data schema and query diagram directly represent the data that parameterizes the optimization model.

The query is evaluated on the acset by computing its limit, which is a kind of generalization of meet. We can confirm that both the acsets and data frame methods return the same number of rows, and look at the output.

```{julia}
ijklm_query = query(ijklm_dat, connected_paths_query)
size(ijklm_query) == size(ijklm_df)
```

```{julia}
ijklm_query[1:5,:] |> markdown_table
```

Now that we know they are equal, we can go ahead and see how fast the acsets version is.

```{julia}
@benchmark let
    ijklm = query(ijklm_dat, connected_paths_query)
    model = JuMP.Model(HiGHS.Optimizer)
    set_silent(model)
    ijklm[!, :x] = @variable(model, x[1:size(ijklm, 1)] >= 0)
    for df in DataFrames.groupby(ijklm, :i)
        @constraint(model, sum(df.x) >= 0)
    end
    optimize!(model)
end
```